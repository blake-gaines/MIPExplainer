{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_dataset\n",
    "from gnn import GNN\n",
    "from pyvis.network import Network\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"MUTAG\"\n",
    "# model_path = \"models/MUTAG_model_new.pth\"\n",
    "dataset_name = \"Shapes_Ones\"\n",
    "model_path = \"models/Shapes_Ones_model.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn = torch.load(model_path, fix_imports=True, map_location=device)\n",
    "nn.device = device\n",
    "nn.to(device)\n",
    "nn.eval()\n",
    "dataset = get_dataset(dataset_name)\n",
    "train_loader = dataset.get_train_loader()\n",
    "test_loader = dataset.get_test_loader()\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(nn, loader, threshold=0):\n",
    "    all_masks = []\n",
    "    ys = []\n",
    "    for data in tqdm(loader):\n",
    "        all_outputs = nn.get_all_layer_outputs(data)\n",
    "        output_vector = torch.concat([o[1].flatten() for o in all_outputs if \"Relu\" in o[0] and \"Lin\" in o[0]])\n",
    "        mask = output_vector > threshold\n",
    "        all_masks.append(mask.to(int))\n",
    "        ys.append(data.y.item())\n",
    "    return torch.stack(all_masks, dim=0).squeeze().detach().numpy(), np.array(ys)\n",
    "\n",
    "train_masks, train_ys = get_masks(nn, train_loader)\n",
    "test_masks, test_ys = get_masks(nn, test_loader)\n",
    "all_masks, all_ys = np.concatenate([train_masks, test_masks], axis=0), np.concatenate([train_ys, test_ys], axis=0)\n",
    "train_masks.shape, test_masks.shape, all_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_numel = [output.numel() for name, output in nn.get_all_layer_outputs(next(iter(train_loader))[0]) if \"Relu\" in name and \"Lin\" in name]\n",
    "mask_indices = dict(zip([name for name in nn.layers.keys() if \"Relu\" in name and \"Lin\" in name], np.cumsum(out_numel)))\n",
    "mask_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start, end = mask_indices[\"conv3_ReLU\"], mask_indices[\"conv4_ReLU\"]\n",
    "# start, end = mask_indices[\"Lin_0_Relu\"], mask_indices[\"Lin_1_Relu\"]\n",
    "start, end = 0, mask_indices[\"Lin_1_Relu\"]\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_masks, unique_train_indices, unique_train_inverse, unique_train_counts = np.unique(train_masks[:, start:end], axis=0, return_index=True, return_inverse=True, return_counts=True)\n",
    "unique_test_masks, unique_test_indices, unique_test_inverse, unique_test_counts =np.unique(test_masks[:, start:end], axis=0, return_index=True, return_inverse=True, return_counts=True) \n",
    "unique_masks, unique_indices, unique_inverse, unique_counts =np.unique(all_masks[:, start:end], axis=0, return_index=True, return_inverse=True, return_counts=True) \n",
    "len(unique_train_masks), len(unique_test_masks), len(unique_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size = 800\n",
    "\n",
    "def get_mask_graph(masks, ys, max_diff = 1):\n",
    "    unique_masks, unique_index, unique_inverse, unique_counts =np.unique(masks, axis=0, return_index=True, return_inverse=True, return_counts=True) #TODO: Problem here\n",
    "    print(\"Unique Masks:\", masks.shape)\n",
    "    print(\"Unique Masks Shape:\", unique_masks.shape)\n",
    "    print(\"Unique Index Shape:\", unique_index.shape)\n",
    "    print(\"Unique Inverse Shape:\", unique_inverse.shape)\n",
    "    print(\"Unique Counts Shape:\", unique_counts.shape)\n",
    "    G = nx.Graph()\n",
    "    for i, mask in enumerate(unique_masks):\n",
    "        mask_tuple = tuple(mask)\n",
    "        mask_hex = hex(int(\"\".join(mask.astype(str)), 2))\n",
    "        mask_indices = np.nonzero(unique_inverse == i)[0]\n",
    "        fig, ax = dataset.draw_graph(data=dataset.data[unique_index[i]])\n",
    "        fig.canvas.draw()\n",
    "        img = Image.frombytes(\n",
    "            \"RGBa\", fig.canvas.get_width_height(), fig.canvas.buffer_rgba()\n",
    "        )\n",
    "        plt.close()\n",
    "        # img = img.resize((node_size*20, round(((node_size*20)/img.size[0])*img.size[1])))\n",
    "        class_proportions = {f\"class_prop_{c}\": np.sum(ys[mask_indices] == c)/unique_counts[i] for c in np.unique(ys)}\n",
    "        G.add_node(mask_tuple, mask=mask, index=i, mask_indices=mask_indices, count=unique_counts[i], hex=mask_hex, image=img, **class_proportions)\n",
    "        for other_node in G.nodes:\n",
    "            bits_different = np.sum(mask ^ other_node)\n",
    "            if 0 < bits_different <= max_diff:\n",
    "                G.add_edge(mask_tuple, other_node, bits_different=bits_different, difference=np.nonzero(mask ^ other_node))\n",
    "            # G.add_edge(mask_tuple, other_node, weight=bits_different)\n",
    "    return G\n",
    "\n",
    "G = get_mask_graph(all_masks, all_ys, max_diff=3)\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "fig = plt.figure()\n",
    "ax=plt.subplot(111)\n",
    "\n",
    "nx.draw(\n",
    "    G, pos, edge_color='black', width=1, linewidths=1,\n",
    "    node_size=node_size, node_color='white', alpha=1,\n",
    "    ax=ax, edgecolors=(0,0,0,0)#labels={n: d[\"class_0_prop\"] for n, d in G.nodes(data=True)},\n",
    ")\n",
    "if \"weight\" in next(iter(G.edges(data=True)))[2]:\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        G, pos,\n",
    "        edge_labels={(u, v): d['weight'] for u, v, d in G.edges(data=True)},\n",
    "        font_color='red', ax=ax,\n",
    "    )\n",
    "\n",
    "# Transform from data coordinates (scaled between xlim and ylim) to display coordinates\n",
    "tr_figure = ax.transData.transform\n",
    "# Transform from display to figure coordinates\n",
    "tr_axes = fig.transFigure.inverted().transform\n",
    "\n",
    "# Select the size of the image (relative to the X axis)\n",
    "icon_size = (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.05\n",
    "icon_center = icon_size / 2.0\n",
    "\n",
    "# Add the respective image to each node\n",
    "for n in G.nodes:\n",
    "    xf, yf = tr_figure(pos[n])\n",
    "    xa, ya = tr_axes((xf, yf))\n",
    "    # get overlapped axes and plot icon\n",
    "    a = plt.axes([xa - icon_center, ya - icon_center, icon_size, icon_size])\n",
    "    a.imshow(G.nodes[n][\"image\"])\n",
    "    a.axis(\"off\")\n",
    "    \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network(height=\"1000px\", width=\"100%\")\n",
    "for i, (node, data) in enumerate(G.nodes(data=True)):\n",
    "    title = f\"{node} \\nCount: {data['count']}\"\n",
    "    for c, p in data.items():\n",
    "        if \"class_prop\" in c:\n",
    "            title += f\"\\nProportion of Class {c[-1]}: {p*100:.0f}%\"\n",
    "    title += f\"\\nSome Graph Indices: {data['mask_indices'][:10]}\"\n",
    "    label = \" / \".join([f\"{p*100:.0f}%\" for c, p in data.items() if \"class_prop\" in c])\n",
    "    count_str = f\"Count: {data['count']}\"\n",
    "    label += \"\\n\" + \" \"*((len(label)-len(count_str))//2) + count_str\n",
    "    G.nodes[node][\"image\"].convert(\"RGB\").save(f\"images/{i}.png\")\n",
    "    nt.add_node(str(node), title=title, label=label , shape='image', image =f\"images/{i}.png\")\n",
    "for u, v, data in G.edges(data=True):\n",
    "    # nt.add_edge(str(u), str(v), title= f\"# Bits Different: {str(data['bits_different'])} \\nDifference: {str(data['difference'][0].tolist())}\")\n",
    "    nt.add_edge(str(u), str(v), title= f\"# Bits Different: {str(data['bits_different'])} \\nDifference: {str(data['difference'][0].tolist())}\", value=1/data['bits_different'])\n",
    "nt.show_buttons()\n",
    "nt.save_graph('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for class_index in range(dataset.num_classes):\n",
    "    records = []\n",
    "    for test_data, test_mask in tqdm(zip([d for d in test_loader if d.y == class_index], test_masks[:, start:end])):\n",
    "        prediction = torch.argmax(nn(test_data).squeeze()).item()\n",
    "        test_mask = test_mask.astype(int)\n",
    "        mask_dists_from_train = np.sum(np.abs(test_mask-unique_train_masks), axis=1)\n",
    "        min_l1_idx = np.argmin(mask_dists_from_train)\n",
    "        min_l1 = mask_dists_from_train[min_l1_idx]\n",
    "        deviations = np.argwhere(np.argwhere(test_mask != unique_train_masks[min_l1_idx]))\n",
    "        records.append({\"Class\": test_data.y.item(), \"Predicted Class\": prediction, \"Min Train Mask L1\": min_l1, \"Min Train Mask L1 Index\": min_l1_idx, \"Deviations\": deviations})\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    print(df[df[\"Predicted Class\"] == class_index][\"Min Train Mask L1\"].mean(), df[df[\"Predicted Class\"] != class_index][\"Min Train Mask L1\"].mean())\n",
    "    print(df[df[\"Predicted Class\"] == class_index][\"Min Train Mask L1\"].mean()/len(test_mask), df[df[\"Predicted Class\"] != class_index][\"Min Train Mask L1\"].mean()/len(test_mask))\n",
    "    px.histogram(df, x=\"Min Train Mask L1\", color=\"Predicted Class\", marginal=\"rug\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utm = unique_train_masks.copy()\n",
    "\n",
    "# def create_tree(m, rows=None):\n",
    "#     if rows is None: \n",
    "#         rows = np.arange(m.shape[0])\n",
    "#     if rows.size == 1 or len(np.unique(m[rows], axis=0)) == 1:\n",
    "#         return m[rows][0].astype(int)\n",
    "#     split_index = np.argmin(np.abs(m[rows].sum(axis=0) - len(rows) // 2))\n",
    "#     branch_0_rows = np.argwhere(m[rows, split_index] == 0).squeeze()\n",
    "#     branch_1_rows = np.argwhere(m[rows, split_index] == 1).squeeze()\n",
    "#     assert(branch_0_rows.size+branch_1_rows.size == rows.size)\n",
    "#     # print(m.shape, branch_0_rows.shape, branch_1_rows.shape)\n",
    "#     # print(branch_0_rows)\n",
    "#     # return\n",
    "#     if branch_0_rows.size == 0 and branch_1_rows.size == 0:\n",
    "#         print(\"Oops\", m[rows, split_index])\n",
    "\n",
    "#     if branch_0_rows.size == 0:\n",
    "#         return {(split_index, 1): create_tree(m, branch_1_rows)}\n",
    "#     elif branch_1_rows.size == 0:\n",
    "#         return {(split_index, 0): create_tree(m, branch_0_rows)}\n",
    "#     else:\n",
    "#         return {(split_index, 0): create_tree(m, branch_0_rows), (split_index, 1): create_tree(m, branch_1_rows)}\n",
    "\n",
    "# tree = create_tree(utm)\n",
    "\n",
    "# def print_tree(t, d=0, depth=0):\n",
    "#     size = 0\n",
    "#     if isinstance(t, dict):\n",
    "#         for k in t.keys():\n",
    "#             print(\" \"*d+str(k)+\": \"+str(t[k]))\n",
    "#             sub_depth, sub_size = print_tree(t[k], d+2, depth+1)\n",
    "#             size += sub_size\n",
    "#         return sub_depth+1, size\n",
    "#     else:\n",
    "#         print(t)\n",
    "#         return 1, 1\n",
    "# print_tree(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurobi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
